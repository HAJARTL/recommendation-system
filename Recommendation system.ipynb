{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70o4V3gH2p05"
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "id": "WT6CEIiXtsqO"
   },
   "outputs": [],
   "source": [
    "#Import  web scraping libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkQsU_Wv3XiW"
   },
   "source": [
    "## Sport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "id": "o48AwsXdSrDI"
   },
   "outputs": [],
   "source": [
    "#the first main field : SPORTS:\n",
    "URLS_Sport=[]\n",
    "Sport_Articles=[]\n",
    "Sport_Titles=[]\n",
    "#URL of Sport Articles\n",
    "URL = 'https://en.wikipedia.org/wiki/Sport' \n",
    "page = requests.get(URL)\n",
    "bsoup = BeautifulSoup(page.content, 'html.parser')\n",
    "rlts = bsoup.find(id='mw-content-text')\n",
    "title=soup.find(id=\"content\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=title.find(\"h1\")\n",
    "#the title of the article\n",
    "t=title.get_text()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "art = rlts.find_all('div', class_='mw-parser-output')\n",
    "content=\"\"\n",
    "for x in art:\n",
    "    for i in x.find_all('p'):\n",
    "      #Extract Article paragraphs\n",
    "      content=content+i.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 43 sports' article\n"
     ]
    }
   ],
   "source": [
    "rlts = bsoup.find_all('div',class_='div-col')\n",
    "URLS_Sport=[]\n",
    "txt=\"https://en.wikipedia.org/\"\n",
    "for x in rlts:\n",
    "  z=x.find_all('a', href=True)\n",
    "for i in z:\n",
    "  #URLS of related articles\n",
    "  URLS_Sport.append(txt+i['href'])\n",
    "print('We have',len(URLS_Sport),\"sports' article\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collecting the articles from the links**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in URLS_Sport:\n",
    "  URL = link\n",
    "  page = requests.get(URL)\n",
    "  soup = BeautifulSoup(page.content, 'html.parser')\n",
    "  results = soup.find(id='mw-content-text')\n",
    "  title=soup.find(id=\"content\")\n",
    "  title=title.find(\"h1\")\n",
    "  t=title.get_text()  #Title of article\n",
    "  Sport_Titles.append(t)\n",
    "  art = results.find_all('div', class_='mw-parser-output')\n",
    "  s=\"\"\n",
    "  for x in art:\n",
    "    for i in x.find_all('p'):\n",
    "      #Extract Article paragraphs\n",
    "      s=s+i.text\n",
    "  Sport_Articles.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(Sport_Articles))\n",
    "print(len(URLS_Sport))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nadB5Wn18gfb"
   },
   "source": [
    "## Society"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the second main field : SOCIETY:\n",
    "URLS_Society=[]\n",
    "Society_Articles=[]\n",
    "Society_Titles=[]\n",
    "#URL of Society Articles\n",
    "URL = 'https://en.wikipedia.org/wiki/Society' \n",
    "page = requests.get(URL)\n",
    "bsoup = BeautifulSoup(page.content, 'html.parser')\n",
    "rlts = bsoup.find(id='mw-content-text')\n",
    "title=bsoup.find(id=\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=title.find(\"h1\")\n",
    "#the title of the article\n",
    "t=title.get_text()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "artictles = rlts.find_all('div', class_='mw-parser-output')\n",
    "content=\"\"\n",
    "for x in artictles:\n",
    "    for i in x.find_all('p'):\n",
    "      #Extract Article paragraphs\n",
    "      content=content+i.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Society_Articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 31 Society' article\n"
     ]
    }
   ],
   "source": [
    "results = bsoup.find_all('div',class_='div-col')\n",
    "txt=\"https://en.wikipedia.org/\"\n",
    "for x in results:\n",
    "  z=x.find_all('a', href=True)\n",
    "for i in z:\n",
    "  #URLS of related articles\n",
    "  URLS_Society.append(txt+i['href'])\n",
    "print('We have',len(URLS_Society),\"Society' article\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collecting the articles from the links**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in URLS_Society:\n",
    "  URL = link\n",
    "  page = requests.get(URL)\n",
    "  soup = BeautifulSoup(page.content, 'html.parser')\n",
    "  results = soup.find(id='mw-content-text')\n",
    "  title=soup.find(id=\"content\")\n",
    "  title=title.find(\"h1\")\n",
    "  t=title.get_text()  #Title of the article\n",
    "  Society_Titles.append(t)\n",
    "  art = results.find_all('div', class_='mw-parser-output')\n",
    "  s=\"\"\n",
    "  for x in art:\n",
    "    for i in x.find_all('p'):\n",
    "      #Extract Article paragraphs\n",
    "      s=s+i.text\n",
    "  Society_Articles.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "print(len(Society_Articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the third main field : Information technology:\n",
    "URLS_Information_technology=[]\n",
    "Information_technology_Articles=[]\n",
    "Information_technology_Titles=[]\n",
    "#URL of Information technology Articles\n",
    "URL = 'https://en.wikipedia.org/wiki/Information_technology' \n",
    "page = requests.get(URL)\n",
    "bsoup = BeautifulSoup(page.content, 'html.parser')\n",
    "rlts = bsoup.find(id='mw-content-text')\n",
    "title=bsoup.find(id=\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=title.find(\"h1\")\n",
    "#the title of the article\n",
    "t=title.get_text()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "artictles = rlts.find_all('div', class_='mw-parser-output')\n",
    "content=\"\"\n",
    "for x in artictles:\n",
    "    for i in x.find_all('p'):\n",
    "      #Extract Article paragraphs\n",
    "      content=content+i.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 13 Information technology' article\n"
     ]
    }
   ],
   "source": [
    "results = bsoup.find_all('div',class_='div-col')\n",
    "txt=\"https://en.wikipedia.org/\"\n",
    "for x in results:\n",
    "  z=x.find_all('a', href=True)\n",
    "for i in z:\n",
    "  #URLS of related articles\n",
    "  URLS_Information_technology.append(txt+i['href'])\n",
    "print('We have',len(URLS_Information_technology),\"Information technology' article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in URLS_Information_technology:\n",
    "  URL = link\n",
    "  page = requests.get(URL)\n",
    "  soup = BeautifulSoup(page.content, 'html.parser')\n",
    "  results = soup.find(id='mw-content-text')\n",
    "  title=soup.find(id=\"content\")\n",
    "  title=title.find(\"h1\")\n",
    "  t=title.get_text()  #Title of the article\n",
    "  Information_technology_Titles.append(t)\n",
    "  art = results.find_all('div', class_='mw-parser-output')\n",
    "  s=\"\"\n",
    "  for x in art:\n",
    "    for i in x.find_all('p'):\n",
    "      #Extract Article paragraphs\n",
    "      s=s+i.text\n",
    "  Information_technology_Articles.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(Information_technology_Articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oECDWSbHE1KX"
   },
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Be0dIHDXuJ32",
    "outputId": "4bb94627-d57d-443e-f38e-c4be38e5e518"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\hajar\n",
      "[nltk_data]     taouil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import libraries:\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "id": "fi8sQg5REev9"
   },
   "outputs": [],
   "source": [
    "#Put all articles in one list\n",
    "Articles = Sport_Articles+Information_technology_Articles+Society_Articles\n",
    "\n",
    "#Put all titles in one list\n",
    "Titles = Sport_Titles+ Information_technology_Titles + Society_Titles\n",
    "\n",
    "#id for each article\n",
    "IDS=[i for i in range(len(Titles))]\n",
    "\n",
    "#Data Cleaning\n",
    "j=0\n",
    "for i in Articles:\n",
    "  #remove \\n\n",
    "   Articles[j]= i.replace('\\n','')\n",
    "   j=j+1\n",
    "\n",
    "j=0\n",
    "for i in Articles:   \n",
    "   #remove \\xa0\n",
    "   Articles[j]= i.replace(u'\\xa0', u' ')\n",
    "   j=j+1\n",
    "\n",
    "j=0\n",
    "for i in Articles:\n",
    "  #remove numbers between box brackets \n",
    "   Articles[j] = re.sub(r'[\\d]', '', i) \n",
    "   j=j+1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "CsaCMa7uGTkk",
    "outputId": "5b8c2dac-8623-4683-d3cd-ce4be3bf08f5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sport of athletics</td>\n",
       "      <td>Athletics is a group of sporting events that i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Animals in sport</td>\n",
       "      <td>Animals in sport are a specific form of workin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Combat sport</td>\n",
       "      <td>A combat sport, or fighting sport, is a compet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Parasports</td>\n",
       "      <td>Parasports are sports played by people with a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Esports</td>\n",
       "      <td>Esports (also known as electronic sports, e-sp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id               Title                                            Article\n",
       "0   0  Sport of athletics  Athletics is a group of sporting events that i...\n",
       "1   1    Animals in sport  Animals in sport are a specific form of workin...\n",
       "2   2        Combat sport  A combat sport, or fighting sport, is a compet...\n",
       "3   3          Parasports  Parasports are sports played by people with a ...\n",
       "4   4             Esports  Esports (also known as electronic sports, e-sp..."
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_art = list(zip(IDS,Titles, Articles))  \n",
    "df = pd.DataFrame(list_of_art, \n",
    "                  columns = ['id', 'Title','Article']) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "id": "DsCLsorctpv2"
   },
   "outputs": [],
   "source": [
    "#Remove punctuation\n",
    "def remove_punc(text):\n",
    "  no_punc=\"\".join([c for c in text if c not in string.punctuation])\n",
    "  return no_punc\n",
    "j=0\n",
    "for i in Articles:\n",
    "  Articles[j]=remove_punc(i)\n",
    "  j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "id": "-eT39u_Xusnp"
   },
   "outputs": [],
   "source": [
    "#Remove special caracters\n",
    "def special_car(text):\n",
    "   no_spe=re.sub('[^A-Za-z]+', ' ', text)\n",
    "   return no_spe\n",
    "j=0\n",
    "for i in Articles:\n",
    "  Articles[j]=special_car(i)\n",
    "  j=j+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlDi6_rBvYga"
   },
   "source": [
    "## Recommender System: Content Filtering \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnWJmNKa0T4U"
   },
   "source": [
    "Content-based filtering uses item features to recommend other items similar to what the user likes, based on their previous actions or explicit feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "id": "FRa3bmkOvjmv"
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qN09vGBfQzjx",
    "outputId": "4260a192-e6ab-49bb-cf46-15a565b1365b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87, 18261)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define a TF-IDF Vectorizer Object and remove all english stop words such as 'the', 'a'\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "tfidf_matrix = tfidf.fit_transform(Articles)\n",
    "\n",
    "#Output the shape of tfidf_matrix\n",
    "tfidf_matrix.shape\n",
    "#tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UkJXoGT0R0DL",
    "outputId": "6b58019b-b227-4083-81ca-da45e726c197"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_sim[0]=  [1.         0.10896574 0.1010486  0.29476586 0.11591366 0.06343361\n",
      " 0.02658566 0.19049361 0.05111374 0.12273154 0.13597706 0.27741599\n",
      " 0.07147864 0.09969474 0.24441656 0.29628144 0.07096864 0.0496307\n",
      " 0.10652029 0.02605264 0.09067048 0.17902173 0.16397516 0.06472516\n",
      " 0.06539499 0.07376079 0.09333416 0.06343361 0.17173531 0.16062747\n",
      " 0.15893825 0.1350639  0.09410964 0.02039966 0.05715712 0.15666865\n",
      " 0.10253231 0.15866254 0.17624257 0.09928937 0.22894511 0.\n",
      " 0.08755289 0.02521156 0.02265962 0.02618007 0.01554636 0.03574614\n",
      " 0.03218516 0.02421343 0.00432385 0.01832035 0.01282084 0.01992668\n",
      " 0.01315563 0.05913104 0.03019397 0.0623857  0.04424077 0.03858413\n",
      " 0.01144055 0.04982322 0.01653015 0.01971796 0.0551715  0.0355307\n",
      " 0.02509048 0.0186507  0.01735797 0.03825398 0.04160232 0.02158229\n",
      " 0.04440897 0.02249279 0.02836579 0.02101482 0.02710831 0.02882266\n",
      " 0.06103568 0.02769618 0.06103568 0.0401598  0.02708704 0.02665887\n",
      " 0.03077731 0.04724931 0.02898181]\n",
      "shape  (87, 87)\n"
     ]
    }
   ],
   "source": [
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "print(\"cosine_sim[0]= \",cosine_sim[0])\n",
    "print(\"shape \",cosine_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "id": "Bk2bgMwkh6OG"
   },
   "outputs": [],
   "source": [
    "#Construct a reverse map of indices and article titles\n",
    "\n",
    "indices = pd.Series(df.index, index=df['Title']).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A86Mqvb5hXro",
    "outputId": "bb0a26ba-47db-424d-9bf6-34bfd0ac54a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Indices\n",
    "indices[90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "id": "ofkxNffokgjf"
   },
   "outputs": [],
   "source": [
    "# Function that takes in article title as input and outputs most similar articles\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    # Get the index of the article that matches the title\n",
    "    idx = indices[title]\n",
    "\n",
    "    # Get the pairwsie similarity scores of all articles with that article\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the articles based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar articles\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the article indices\n",
    "    article_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar articles\n",
    "    return df['Title'].iloc[article_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5PjjNOfk4PB",
    "outputId": "f37f80f0-b980-4794-b22d-df8d4a7ceb9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31         Sports marketing\n",
       "3                Parasports\n",
       "7       International sport\n",
       "0        Sport of athletics\n",
       "22              Sports club\n",
       "11        Multi-sport event\n",
       "38               Team sport\n",
       "14            Olympic Games\n",
       "28    Sports governing body\n",
       "21         Sport psychology\n",
       "Name: Title, dtype: object"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's test with title = \"Women's sports\"\n",
    "get_recommendations(\"Women's sports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhC0S7pOz3s8"
   },
   "source": [
    "We get the 10 most similar articles to Women's sports Article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-favYlRz6xv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Project1: Recommender System.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
